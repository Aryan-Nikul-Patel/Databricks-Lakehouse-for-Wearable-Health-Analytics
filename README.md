# Wearable Device Lakehouse Platform

## Introduction

Your company manufactures a wrist‑watch style wearable that continuously streams health parameters to your servers. This project covers designing and building a data engineering system to ingest, process, and serve that data.

## Data Sources

1. **Device Registration**  
   - Captured at retail sale  
   - Fields: `user_id`, `device_id`, `mac_address`, `registration_timestamp`  
   - Stored in a cloud database

2. **User Profile Change Data (CDC)**  
   - Generated by the mobile app on profile **create**, **update**, or **delete**  
   - JSON records sent to a single Kafka topic

3. **Heart‑Rate (BPM) Events**  
   - Continuous, high‑volume stream of `{ user_id, device_id, bpm, timestamp }`  
   - Sent to a Kafka topic

4. **Facility Login/Logout Events**  
   - Scanners at partner fitness/health centers detect device entry/exit  
   - `{ user_id, device_id, event_type (login|logout), timestamp }` sent to a Kafka topic

5. **Workout Session Events**  
   - Push‑button on device records `start` and `stop` of workouts  
   - `{ user_id, session_id, event_type (start|stop), timestamp }` sent to a separate Kafka topic

## Requirements

- **Lakehouse Platform**  
  Design and implement a Medallion Architecture (Bronze → Silver → Gold).

- **Data Ingestion**  
  Collect and ingest all five datasets from source systems into your platform.

- **Analytics‑Ready Gold Tables**  
  1. **Workout BPM Summary**  
     - Daily per‑user workout session aggregates  
     - Calculate minimum, average, and maximum heart rate during each workout  
  2. **Gym Summary**  
     - Per‑facility daily summary  
     - Total visits, time in facility, and actual exercise duration

## Architecture

### Medallion Layers

- **Bronze**: Raw, append‑only ingestion of each source  
- **Silver**: Cleansed and conformed tables (e.g., CDC merges, session alignment)  
- **Gold**: Business‑ready aggregates for downstream consumption

## Data Governance

Implement audit trails, access controls, security, and data lineage tracking throughout the pipeline.

## Design Goals & Best Practices

- **Environment Isolation**: Dev, Test, and Production  
- **Decoupled Workflows**: Separate ingestion from processing; support both batch and streaming  
- **Automation**: Integration tests and deployment pipelines for Test and Prod

---

*More details and implementation steps to follow as additional project information is provided.*  
