# Wearable Device Lakehouse Platform

## Introduction

A company manufactures a wrist‑watch style wearable that continuously streams health parameters to your servers. This project covers designing and building a data engineering system to ingest, process, and serve that data.

## Data Sources

![](https://github.com/Aryan-Nikul-Patel/Databricks-Lakehouse-for-Wearable-Health-Analytics/blob/main/resource/arch.png)

1. **Device Registration**  
   - Captured at retail sale  
   - Fields: `user_id`, `device_id`, `mac_address`, `registration_timestamp`  
   - Stored in a cloud database

2. **User Profile Change Data (CDC)**  
   - Generated by the mobile app on profile **create**, **update**, or **delete**  
   - JSON records sent to a single Kafka topic

3. **Heart‑Rate (BPM) Events**  
   - Continuous, high‑volume stream of `{ user_id, device_id, bpm, timestamp }`  
   - Sent to a Kafka topic

4. **Facility Login/Logout Events**  
   - Scanners at partner fitness/health centers detect device entry/exit  
   - `{ user_id, device_id, event_type (login|logout), timestamp }` sent to a Kafka topic

5. **Workout Session Events**  
   - Push‑button on device records `start` and `stop` of workouts  
   - `{ user_id, session_id, event_type (start|stop), timestamp }` sent to a separate Kafka topic

## Requirements

- **Lakehouse Platform**  
  Design and implement a Medallion Architecture (Bronze → Silver → Gold).

- **Data Ingestion**  
  Collect and ingest all five datasets from source systems into your platform.

- **Analytics‑Ready Gold Tables**  
  1. **Workout BPM Summary**  
     - Daily per‑user workout session aggregates  
     - Calculate minimum, average, and maximum heart rate during each workout  
  2. **Gym Summary**  
     - Per‑facility daily summary  
     - Total visits, time in facility, and actual exercise duration

## Architecture

![]([URL_or_relative_path_to_image](https://github.com/Aryan-Nikul-Patel/Databricks-Lakehouse-for-Wearable-Health-Analytics/blob/main/resource/arch2.png))

### Medallion Layers

- **Bronze**: Raw, append‑only ingestion of each source  
- **Silver**: Cleansed and conformed tables (e.g., CDC merges, session alignment)  
- **Gold**: Business‑ready aggregates for downstream consumption

## Data Governance

Implement audit trails, access controls, security, and data lineage tracking throughout the pipeline.

## Design Goals & Best Practices

- **Environment Isolation**: Dev, Test, and Production  
- **Decoupled Workflows**: Separate ingestion from processing; support both batch and streaming  
- **Automation**: Integration tests and deployment pipelines for Test and Prod

---

## Solution Design

![Alt Text](https://github.com/Aryan-Nikul-Patel/Databricks-Lakehouse-for-Wearable-Health-Analytics/blob/main/resource/arch4.png)

Before diving into the Lakehouse, we need to satisfy two **operational** requirements that keep the upstream systems working.  These are **not** part of the analytical pipeline and will be handled by a separate team.  We decouple them from the Medallion‑architecture Lakehouse to keep operational and analytical workloads isolated.

### 1. User Profile CDC → Cloud Database

- **Source:** Kafka topic `user_profile_cdc`  
- **Why:** The mobile app reads profile data from the cloud database, not Kafka  
- **What to do:**  
  1. Build a small micro‑service (or Spark Structured Streaming job) that subscribes to `user_profile_cdc`.  
  2. On **create** or **update** events, upsert the JSON payload into the user profile table in the cloud database.  
  3. On **delete** events, remove or deactivate the user record in the database.  

### 2. Gym Login/Logout → Operational Database

- **Source:** Kafka topic `facility_access`  
- **Why:** The gym’s front‑desk application needs a consolidated view of who’s inside and when they left  
- **What to do:**  
  1. Consume **login** and **logout** events from `facility_access`.  
  2. Match each login with its corresponding logout (e.g., by `user_id` + session window).  
  3. Emit a single record per visit:  
     ```json
     {
       "user_id": "...",
       "facility_id": "...",
       "login_timestamp": "...",
       "logout_timestamp": "..."
     }
     ```  
  4. Upsert these visit records into the gym operations database.

---

### Decoupling Operational vs. Analytical Workloads

- **Operational pipelines** (steps 1 & 2 above) run independently and keep day‑to‑day applications functioning.  
- **Analytical Lakehouse** (Bronze → Silver → Gold) only consumes the raw Kafka topics and the device registration database—**not** the operational databases.  

With the operational side covered, we can now focus on designing our Lakehouse to process **all five** raw data sources for analytics.  


---

## Storage Layer Design

We’ll use **Azure Data Lake Storage Gen2** as the foundation for our Lakehouse. All data and metadata live in ADLS Gen2, giving us:

- **Spark & Delta Lake compatibility**  
- **Cost‑effective, pay‑as‑you‑grow** blob storage  
- **Fine‑grained access controls** via Azure RBAC  
- **Encryption at rest & in transit**  
- **Redundancy & cross‑region replication** for high availability  
- **Filesystem‑like directory structure** over blob storage  

---

### Container & Directory Layout

![Alt Text](https://github.com/Aryan-Nikul-Patel/Databricks-Lakehouse-for-Wearable-Health-Analytics/blob/main/resource/arch5.png)

We provision **three** ADLS Gen2 containers per environment (Dev / Test / Prod), plus one shared Metastore container:

| Container Name           | Purpose                                                       |
|--------------------------|---------------------------------------------------------------|
| `esbit-metastore-root`   | Unity Catalog / Hive Metastore metadata (shared across envs)  |
| `esbit-managed-dev`      | Managed Delta tables for Dev (Bronze / Silver / Gold layers)  |
| `esbit-unmanaged-dev`    | External/raw data & streaming checkpoints for Dev             |

